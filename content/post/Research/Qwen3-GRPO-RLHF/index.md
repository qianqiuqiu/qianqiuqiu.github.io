---
title: Qwen3 GRPO验证实验
date: 2026-03-01
image: pig.png
categories:
  - Research
tags:
  - Research
---

这个项目大致是迷你 DeepSeek-R1-Zero的操作，看看影响如何。

用 GRPO 强化学习算法对 Qwen3-1.7B-Base 进行数学推理训练，从零复现 DeepSeek-R1-Zero 的核心机制，验证"无需 SFT 监督数据、纯靠奖励信号即可涌现出 Chain-of-Thought 推理能力"这一结论。

DeepSeek-R1-Zero 论文表明，大型语言模型可以**不经过监督微调（SFT）**，仅通过强化学习奖励信号，自发学会逐步推理。本项目在小模型（1.7B）和小数据集（GSM8K，约 7500 条小学数学题）上复现该思路，验证其可扩展性与工程可行性。都是确定的答案，RLVR.

PPO 需要一个额外的 Critic（价值网络）来估计优势函数，训练成本高。GRPO 的做法是：对同一个 prompt 采样 **G 个回复**，用这 G 个回复的奖励均值和方差直接归一化出优势值，**完全去掉 Critic**。

同时加入 KL 散度惩罚，防止策略模型偏离参考模型太远，保证训练稳定性。β设置成0.001，设得小是因为本实验从 Base 模型（非 Instruct）出发，模型初始没有遵循格式的倾向，需要给策略更大的"自由度"去探索，相对宽松的 KL 约束有助于模型更快学会推理格式。


reward/mean 应随训练步数稳步上升，代表答对率在提高

reward/std  反映组内回复的多样性，若过早趋近 0 说明策略过于保守（退化为重复同一种回答）

局限性在 1.7B 参数 + 500 步，效果远不及 DeepSeek-R1，主要作为原理验证，事实上后期验证对比实验中也发现：
  Qwen3 1.7B BASE                 55.0%   基座模型无SFT
  GRPO-trained (ours)             80.0%   本实验训出，500 步 RL
  Instruct (no think)             58.5%   官方对话模型，标准推理
  Instruct + /think               64.0%   官方模型开启 CoT 思考链

think确实帮助提升了能力，但官方模型并不是特化的RL训练，反而不如我们这简陋的实验，还有另一部分原因是compare里面1024token的截断设置，让think模型在还在思考的时候就被截断当作没答对处理。程序抓到了错误的结果。


还有就是是单卡训练，非常慢 NUM_GENERATIONS = 4  ，如果用多卡可以提速，使用VLLM分配显存。

训练RLVR只有结果，没有PRM过程奖励来约束过程，可能学到乱猜加正确结果。

泛化性有待提高，只在 GSM8K 上训练和评估。


## 改进

过程奖励打分，提高学习比较和约束。
更大的NUM——generation，方差估计更准确，但显存需要更高，训练梯度下降更平滑。
RLVR+sft:SFT先warm-up（极少量），进行回答约束。再做RL更稳定。
VLLM：管理显存，提高并行，加速训练，提高稳定性。



 