---
title: "转码学习day3"
date: 2026-01-23
categories: 
	-study
tag: 
	-study
---

今天睡了9个小时起床，吃了个饭下午两点开始学习。

把昨天transformer最后组装的一点代码看完，开始刷力扣。

lc：21、104(lfs) 、75、46 、78

晚上尝试从数学上推导了一下RoPE和Sin PE。

Sin PE低维捕捉长距离的相对位置，高维捕捉临近位置，因为分母大小不同，这是直观理解，但是实际效果上可能并不如理想那么好，原因有很多。

RoPE把Q K 当成复数，不同pos旋转角不同， 旋转矩阵具有正交性和群性。

​			$R(\alpha)^\top=R(\alpha)^{-1}=R(-\alpha)$                                      $R(-\alpha)R(\beta)=R(\beta-\alpha)$

二者都采用了两两分组d_model维向量（先不考虑head和dk，写着d_model不影响理解问题本质）。

二者都能外推，但是理解上不同。

看到了RoPE变换的优越性和唯一性：

如果有以下四个假设：1位置通过 **对 Q、K 的变换** 注入 2attention score 是**内积**形式（我想这点应该不太容易改变了）3无参数 / 不随训练学习位置矩阵（我觉得位置能表示就好，含参数用来训练学习位置关系是否有需要呢，我暂时不太能想到它的必要） 4 平移等变性    $S_{i+t,j+t}=S_{ij}\quad\Rightarrow\quad\text{只依赖 }j-i$

则可以推导出来对Q K的变换必须是正交变换，而且必须具有群性质，才能满足上面的假设。

数学上所有有限维、连续、实正交表示都可分解为若干个 **二维不可约旋转表示**（加上可能的 1 维平凡表示）(我没去看这段证明，日后再补)

说人话就是满足1-4假设，变换就必须在若干二维子空间里，以与位置成线性的角度，做正交旋转，而这就是RoPE。

xPos / YaRN 本质仍是 RoPE，只是角度缩放或重参数化。

下面来正儿八经分开说一下RoPE和Sin PE。



attn后，PE相对位置信息（Wq Wk x=e+p 。*query* @ *key*展开到e和p层级的话共四项，相对位置信息保存在最后一项）被破坏，因为$\begin{pmatrix}\sin a\\\cos a\end{pmatrix}^\top\begin{pmatrix}\alpha&\beta\\\gamma&\delta\end{pmatrix}\begin{pmatrix}\sin b\\\cos b\end{pmatrix}$，若要满足$p_i^\top Mp_j=g(j-i)$，则**M 必须按P随着位置变换同样的频率分块，且每个 2×2 块必须是旋转群的交换子**。

这对于一个用于训练梯度下降的Wq Wk是不可能满足的。而Sin PE在初版transformer之所以可行，是因为上面括号里说的4项中，除了保存相对位置这一项外，其他位置还保留了足够的“绝对位置线索 + 局部统计偏置”，所以对于翻译这么个对相对位置要求没那么高的活计来说，它已经足够优秀。score计算的相对位置“破坏”在翻译里不致命，翻译的主要难点是词义对齐、长程依赖替代 RNN。精细相对距离建模在翻译上并不是那么care的一个问题，这个问题在大模型性能上更重要，而且不是一般重要。

虽然我吐槽了SinPE这么多，但实际上它已经算是**爆杀**了RNN 的隐式位置、CNN 的有限 receptive field。



RoPE是在 Q,K 上以“旋转”形式注入位置，那么 attention 的点积会天然只依赖**相对位置**，而不是依赖绝对位置i,j 的某种脆弱结构。

核心就是它的群性和正交性组合	$(R(\alpha)u)^\top(R(\beta)v)=u^\top R(\beta-\alpha)v$



在这里我突然有了个疑惑既然Sin PE受到线性变换影响，那么线性变换完之后再对Q K矩阵加sin 位置信息可以吗？是不行的，这就涉及到信息耦合的问题。

线性变换或 score之后再加 sinusoidal 位置信息，最多只能得到“位置 bias”，而无法恢复 SinPE 原本依赖的相对位置内积结构。



### 方案 A：在 score 上直接加 SinPE

$S_{ij}^{\prime}=q_i^\top k_j+g(i,j)$    等价于：attention score = **内容相关项** + **位置偏置项**  

无法表达：“**这个 token 在这个相对位置时，更相关**”

### 方案 B：对 Q、K 投影后再“加”位置

$\tilde{q}_i=q_i+p_i,\quad\tilde{k}_j=k_j+p_j$（我疑惑的是这种）

### 它的问题是

1. **被其他三项淹没**
   - $q_i^\top k_j$：内容主导
   - $q_i^\top p_j,p_i^\top k_j$：内容–位置混合噪声
2. 相对位置项是**加性的、弱的**
3. 模型没有结构性理由去“尊重”这条恒等式

本质上仍然是：

> **SinPE 的相对位置能力只是一个“脆弱的加法项”**

不写了，到睡觉的点了，总结一下

SinPE 是**加法式注入**，attention 真正使用的是**乘法**

SinPE在Embedding加入，耦合了但是会被线性变换破坏。上面这俩则没有耦合，表达能力差。

RoPE与内容耦合，不被线性变换破坏。

> **在要求位置编码通过对 Q,KQ,KQ,K 的无参数线性变换注入、
并保持 attention 的内积形式与平移等变性的前提下，
RoPE（或与之等价的二维旋转表示）在结构上是唯一可行的方案。**

明天补一下YaRN，今天效率好低，快睡觉了才努力总结了上面的内容。

