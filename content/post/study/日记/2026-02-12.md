---
title: 转码Day 23：
date: 2026-02-12
categories:
  - diary
tag:
  - diary
math: true
---
今天起床感觉精神多了，虽然还没完全好，但状态很明显好多了。

lc: 560、437

估计这几天要开始水第二个项目了，最近看了很多概念性的博客和论文报告，感觉学到了很多，大体上感觉自己越来越了解当今各家公司都在干什么了。

GLM5只开放给Max和Pro用户，不给lite用户，而且订阅价格来了一波大涨似乎激起了一些讨论，只能说再等等吧，真心希望能到达sonnet 4.5以上水平，我会和期待ds尚未发布的大招一样期待GLM5的。

顺便提一句Kimi k2.5还可以，写代码这方面是挺够用了，但如果用来对话学习的话，有的时候还是差点意思。

看了一篇讲anthropic在AI可解释性方面发的几篇论文的blog，但似乎由于a社一贯的闭源方针，这方面工作受到了较多质疑。

他们起点是在2022年9月发的那篇 Toy Models of Superposition

大语言模型里的神经元几乎都是多义的，和人类的想法不一样。

用小型ReLU网络做了系统性实验 特征是稀疏的时候，模型会把远超自身维度数量的特征”压缩”存储到同一组神经元里

篇论文还发现了一个挺惊人的东西 **叠加现象和对抗样本之间可能存在深层关联**。
因为模型内部的特征本来就是叠在一起的 微小的扰动就足以把一个特征的信号”拨”到另一个特征的方向上去。

光看单个神经元没用，得找到一种方法把那些叠在一起的特征拆开来。

2024年5月，**Scaling Monosemanticity** 出来了 是整个可解释性领域近几年最有影响力的工作，没有之一。

核心方法是**稀疏自编码器**  在模型的中间层接一个”分解器” 每层的激活向量强制拆分成一大堆稀疏的、独立的分量  这次不是在toy model上做的，是直接上了**Claude 3 Sonnet** 。

可解释的特征有3400万个多。

有“金门大桥” 特征，“多种代码安全漏洞”，还有跨语言跨模态等等。 直接引出了一个可能性——如果我们能定位这些特征 ，理论上是不是就能监控它们？甚至……调低它们？

调高”金门大桥”特征，就得到那个著名的”_Golden Gate Claude_“聊什么都扯到金门大桥，自称是金门大桥的化身。**第一次实现了对大语言模型内部概念的精确定位和可控干预**

这篇论文还建立了**scaling law**，通过控制字典大小和训练数据，特征的质量和数量是可预测的。也许只是理论上的，实践效果没人知道到底能不能预测。


2025年3月，同时发出的两篇论文 **_Circuit Tracing** 和 **On the Biology of a Large Language Model_**。

Circuit Tracing解决**这些特征之间是怎么互动的**。 又搞了一个叫**跨层转码器（Cross-Layer Transcoder, CLT）** 的东西 可以追踪每一个特征是怎么被激活的、它又去激活了哪些下游特征、最终怎么汇聚成输出的。

通过归因图技术，追踪过程被可视化 这并不是一个事后合理化的解释，而是**实时追踪到的信号流**。

另一篇使用这个技术，发现了一些细节：

claude 有超语言的能力，同一个句子不同语言，模型中间层被激活特征几乎完全一致，最后几层才分叉到不同语言。对于AI来说，”思维”是高于语言存在的。

**一个每次只能生成一个token的模型会做规划** 模型在生成一行诗的**第一个词之前**，就已经在内部激活了那行诗**末尾的押韵词**。

前瞻性规划能力在模型架构层面是”不应该存在”的，但它确实在训练里产生了。

更好地解释了幻觉， 一道困难的数学题，同时附上一个错误的”提示”答案。两股力量在拉扯：一边是试图从数学逻辑出发正确推理的特征链路 另一边是”用户给了一个提示，应该顺着它说”的迎合性链路。

归因图证明了思维链的那些推理步骤并不是真正的决策路径，就好像上面的诗词生成规划，韵脚早早就决定了一样，思维链本质是假的，是一种语言风格微调的结果而已。

**Claude做数学的方式和人类完全不同** 实际内部策略更像是**先做一个模糊的量级估算（大概90多），然后单独精确计算个位数（5+6=…5），最后拼起来**

**多步推理不需要思维链也能在内部完成**  归因图显示模型在**单次前向传播中**就完成了多步推理——先识别出症状组合，然后激活相关疾病特征，再权衡不同诊断的可能性，最终选择最匹配的那个。

这个文章主要是做的MLP方面的工作。另一个注意力机制也有突破 多个不同的”注意力特征”被压缩存储在同一个注意力头里，跨头甚至跨层分布。 注意力机制中”看哪里”（QK条件，决定注意力聚焦在哪个位置）和”传什么”（OV条件，决定传递什么信息）这两个功能是**耦合在一起的**，不是以前认为的相对独立。 把注意力头的行为也纳入归因图体系 借此监控模型在对话过程中人格是否发生了漂移 识别哪些训练数据导致了人格偏移，甚至在训练过程中阻止不良人格特质的形成。

然后就到最后的有些玄学的范畴了 训练一个专门的AI来解读数百万个特征。把目标模型某一层的激活向量直接喂给它，然后用自然语言问它各种问题

26年1月
对模型的人格空间做了主成分分析，发现了一个叫做 **“Assistant Axis（助手轴）”** Assistant Axis在**预训练模型中就已经存在了** 开发了一种叫 **“激活封顶”** 的技术 效果是模型变得更稳定，不容易被越狱，也不容易在长对话中人格漂移。

看起来这个4年的可解释性工作很棒很无敌，但其实还是有很多不太合理的地方。

最明显的就是用AI解释AI，你没法采用循环论证来说服所有人，所以结果可靠性有待提高，哥德尔不完备定理在这种场景下很难避免。

“金门大桥特征” 大概只有10%的激活真的和金门大桥有关，剩下90%的时候它在响应完全不相关的东西。

Circuit Tracing的核心方法是用CLT构建一个”_替代模型_“来近似原模型 大约50%的情况下复现基础模型的输出，研究能这么研究，但真的没法用。

实验规模最大就到Claude 3.5 Haiku，规模不同可能理解链路久完全不同。毕竟“涌现”的出现就是一个意外，SAE的可解释性不一定可以泛化到现如今主力模型。

Anthropic在可解释性方面开源了，虽然它们模型从不开源。 2025年5月他们把circuit tracing的工具全部开源了，放在GitHub上。

晚上吃完饭感觉甩头会疼，这是有点发烧了按以往经验看，睡了一个半小时起来做了两题力扣，水了好久论坛，又是无所事事的一天，越临近过年，效率越低，唉，明天继续。





