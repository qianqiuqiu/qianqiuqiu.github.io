---
title: 转码 Day 20：
date: 2026-02-09
categories:
  - diary
tag:
  - diary
math: true
---

### Multi-head Latent Attention（MLA）的核心原理

**核心思想**：不存完整的K/V，只存"压缩笔记"，用的时候再"解压"

**与GQA/MQA的区别**：
- GQA/MQA：共享K/V内容
- MLA：共享压缩表示，但解压后每个头不同

**为什么能压缩？**
- 自然语言的KV信息集中在低维流形上
- 类似用经纬度（2维）定位地球表面，不需要完整3D坐标

**信息瓶颈视角**：
- 压缩极限 = 再压缩就会丢失关键预测信息的拐点
- DeepSeek-V2实证：512维是甜点，256维开始明显下降

**为什么线性投影足够？**
- 注意力本身是双线性操作（Q·K）
- 线性变换保持内积结构，不会扭曲信息

**自适应压缩方向**：
- 分层：底层高维、顶层低维
- 重要性筛选：只存重要token的KV
- 实际中固定压缩的MLA已足够好


然后开始看 deepseekv3 的技术报告。a large Mixture-of-Experts (MoE) model with 671B parameters, of which 37B are activated for each token.

这里的 37B 不是一个专家的参数大小，是它计算时实际用到的参数总量，是其"用部分参数换整体容量"设计的核心体现。每个 token 可能被路由器分配给 2 个专家，加上共享层的参数，总和约为 37B。

multi-token prediction training objective：输入一段文本后，需要一次性预测出连续的多个输出 token（而不是只预测下一个 token）的训练方式。

多 token 预测主要是训练阶段的目标设计，推理时仍然是自回归生成（一个 token 接一个 token 生成），但训练时的多 token 目标会让模型在推理时表现更好。

多 token 预测的 loss 是连续多个 token 的交叉熵损失之和，梯度基于总 loss 反向传播，让模型从整体语义出发学习，而非仅关注单个 token 的局部预测。

多 token 预测和梯度累计是正交的技术，单个样本（如一个句子）内部的多个预测 token，梯度累计是多个样本（如多个句子）的梯度。

还用 R1 思维链模型等推理能力蒸馏来改善 v3。

ds 当时的表现几乎可以说是达到了 sota，只在工程相关任务，DeepSeek-V3 的表现略低于 Claude-Sonnet-3.5，但它仍然大幅领先于所有其他模型，展示了其在各种技术基准上的竞争力。

如今这么久过去了，claude 一家已经有独步天下的态势了，我指的不只是能力还有贵的无法理解的价格。。。。期待今年的 V4 能有一些新东西。

虽然现在国内模型在一年之后已经不太打得过国外了，但说到底最坏的情况也就是一年的差距了，虽然看起来能力上出现了鸿沟，但我觉得其实是可以乐观一点的，最近的 KIMI2.5 其实就挺聪明了，glm4.7 也还成。

比起这个，我本科的 IC 行业，那才是天大的鸿沟，我得了一种看到拿 70 亿募集资金理财的领头羊摩尔线程就想笑的病。

DeepSeek-V3 的基本架构，特点突出的是 Deepseekmoe 和 MLA：

DeepSeekMoE 额外引入了辅助无损耗负载平衡策略（Wang et al., 2024a），以减轻因确保负载平衡而导致的性能下降。

MHA 上用了 MLA 实现高效推理（对注意力键和值进行低秩联合压缩，生成时只缓存高亮标记向量）。

lc:105