---
title: 转码 Day 17：对比实验与评估
date: 2026-02-06
categories:
  - diary
tag:
  - diary
math: true
---

## 计划概述

今天计划完善昨天的任务，开展评估工作。主要进行三组对比实验，以验证不同训练方法的有效性。

---

Token embedding and LM head are frozen in all settings.

## A. Baseline：不微调（Prompt + Zero-shot）

### 原理说明

**Zero-shot** 指不使用任何该任务的训练样本进行参数更新，仅依靠预训练模型本身的能力和提示词。

实验目标：**证明 LoRA 微调确实带来了性能增益**。

### 实现方式

#### 1. 生成式

让模型直接生成"正向"或"负向"的文本输出。

#### 2. 打分式（推荐）

计算模型对"正向"和"负向"这两个候选答案的 log-likelihood，选择概率更大的标签。

> **打分式的优点**：无需担心生成结果跑偏，结果稳定且可复现。

### 关于生成式的疑问

对于分类任务，采用生成式方法容易出现结果跑偏的问题。

### Label Scoring 工作机制

**打分式**的原理是：

> 不让模型自由生成一段话，而是把每个候选标签（如"正向""负向"）当作"可能的下一段输出"，计算模型对它们的概率/对数概率，选择概率更大的结果。

这正是语言模型的本职工作：**给定上下文，对"接下来出现什么 token"分配概率**。分类任务本质上是将输出空间限制为少数几个候选字符串。

### Logits 手算实现步骤

这是最通用、可控的实现方式：

1. **分别拼接候选标签**
   - prompt + "正向"
   - prompt + "负向"

2. **前向传播**：对每个拼接后的完整序列运行一次前向传播，得到每个位置的 logits

3. **计算分数**：只取"标签部分 token"的 logprob，求和得到该标签的分数

4. **预测决策**：选择分数更高的标签作为预测结果

由于没有训练任何参数，这属于严格的 zero-shot 方法。

### 标签词偏置问题

**核心问题**："标签词的选择会不会影响 zero-shot 结果？"

答案是：**会不一样**。如果使用"正向/负向"、"好/坏"或"positive/negative"，结果会产生差异。这确实影响 baseline 的客观性。

在学术界，这一问题被称为：

> **Label / Verbalizer Bias（标签词偏置）**

### Baseline 严谨性的思考

**关键洞察**：baseline 的"严谨性"取决于其用途。

#### 严谨性分析

在固定 prompt 和标签词的前提下，zero-shot 与微调模型在"同一决策接口"下进行对比：

- Zero-shot 使用 prompt + label scoring
- LoRA / head 模型最终也输出同一组标签词

比较的是：**模型参数是否让"正确标签的概率"系统性提高**。

在这种"相对比较"的语境下，该方法作为 baseline 是合理的。

---

## B. Head-only（非 Zero-shot，但有参考价值）

**方法**：冻结 base Qwen（不加 LoRA）+ 训练一个分类头

**技术名称**：Linear probe / head-only fine-tuning

注：该实验已在昨天完成。

---

## C. LoRA + Head（主模型）

**方法**：base Qwen + LoRA（更新 LoRA 参数）+ 分类头（通常也参与训练）

### 对比实验的价值

通过三组对比可以清晰地定位增益来源：

- **Head-only vs LoRA+Head**：增益来自 LoRA，而非仅靠分类头
- **Zero-shot vs Head-only**：验证任务本身是否需要监督才能有效分类

---

## 实验结果

### 工具说明

实验A使用了 **Claude Opus 4.6**。模型性能优异

- 高并发场景下无限速问题
- 代码生成准确度高，自我纠错频率低
- 唯一缺点是成本较高（通过 GitHub Copilot 学生认证使用）

在实现 baseline（Prompt + Zero-shot）任务时，Opus 4.6 仅用一分钟即生成完美代码，大幅节省了调试时间。

### 性能对比

| 模型 | Accuracy | F1 | AUC-ROC |
|------|----------|-----|---------|
| **A. Zero-shot (打分式)** | 0.880 | 0.882 | 0.930 |
| **B. Head-only (冻结+分类头)** | 0.512 | 0.675 | 0.551 |
| **C. LoRA + Head (微调)** | **0.938** | **0.937** | **0.977** |

### 结果分析

#### Head-only 性能异常

偷懒的Head-only 的分数最低，这一结果是可以预期的。该方法将不匹配的模型和分类头强行组合，存在本质问题。

#### Head-only 的价值

尽管性能不佳，该实验仍有重要发现：

1. **LoRA 不仅仅是微调了表示，它根本改变了 hidden states 的分布**
2. **分类头和 LoRA 存在相互依赖关系，不能单独提取**
3. **Zero-shot 的有效性证明 Qwen 本身对中文情感有很强的先验知识**

### 进一步思考

是否应该做一个**冻结所有参数，只训练分类头**的极端head-only实验？

在大模型场景下，这一问题无需重点考虑，因为大模型的"头"和词嵌入共享权重，这是为了保持输入输出的一致性。

冻结所有参数仅训练分类头，性能很可能介于 A 和 B 之间，甚至接近 B。

---

## 深度分析

### Zero-shot 有效性的根本原因

Zero-shot 实际上不是"没有训练"，而是利用了**预训练时学到的 token-level 对对齐能力**：

- **LM Head（词表投影）和 hidden states 的高度适配**
- 情感分类任务在中文语料中属于**高频语义轴**

### LoRA 与分类头的机制差异

**LoRA**：通过旋转 / 拉伸 / 重排 hidden states，使目标类别在低维线性空间中可分。

**分类头**：仅作为"读出器"提取分类结果。

实验 B 正好提供了反例证明。

### 实验结论

- **A（Zero-shot）**：已经是一个非常强的 baseline
- **C（LoRA + Head）**：明确的最优解
- **极端反例**：添加"冻结所有参数只训练头"的实验只会印证已知结论，意义有限

---

## 延伸方向

### 有意义的对比方向

除了当前的三组实验，还有一个有价值的对比：

**LoRA + Label Scoring vs LoRA + Classifier Head**

### 1) LoRA + Label Scoring（生成式分类接口）

**特点**：
- 不训练或不使用 `Linear(d→2)` 分类头
- 仍然使用 **LM Head（词表投影、与输入词嵌入权重共享）**
- 预测时仅比较两个候选标签字符串（如"正向""负向"）的 **logprob**

**LoRA 的作用**：让模型在给定输入后，更倾向于提高正确标签 token 的概率。

### 2) LoRA + Classifier Head（判别式分类接口）

**特点**：
- 在某层 hidden state 上接一个分类头（通常取最后一层 `h`，或 pooled 表示）
- 使用交叉熵训练该头（同时可训练 LoRA）
- 输出是 `softmax(W h + b)`，类别空间为 2 维，不经过词表

**LoRA 的作用**：改变 hidden states 的分布，使得线性头能够有效区分。

### 本质差异

- **Label Scoring**：token 级别的生成概率比较
- **Classifier Head**：表征到类别的额外映射（新增参数 + 新决策边界）

---

## Embedding 权重问题

### 标准做法

两种方法的标准做法是：**Embedding 矩阵不变**。

目的是保证"正向 / 负向"这两个 token 的向量本身不会漂移。

### 需要区分的关键问题

我们需要明确区分：
- 是 LoRA 让模型"更理解情感"？
- 还是 embedding 把"正向"这个词硬拽过去了？

### 允许 Embedding 更新的风险

如果允许 embedding 更新，模型可以用极低成本：

- 把"正向"的 embedding 向所有正样本 hidden 靠拢

这**不是"理解任务"**，而是**篡改标签语义**。在二分类任务和小数据场景下，这一问题几乎必然发生。

### 对 Head-only 的启示

如果允许 embedding 训练，Head-only（冻结 base + 只训 head）甚至可能"复活"，但这并非是因为 head 的判别力提升，而是语义空间整体发生了漂移。

### 训练策略对比

#### 全量 SFT

- **不要求冻结 embedding**
- 在参数集合上与 pretrain 相同
- 对 embedding 的梯度较弱（不是"冻结"，只是**梯度不显著**）

#### PEFT / LoRA

- **通常冻结 embedding**
- 目的是减少自由度，控制因自由度增多而引入的归纳偏置 


---

## 实验补充：LoRA + Label Scoring

晚上十一点，想着打游戏但是突然不知道玩什么，似乎暂时网游已经不太能引起我的兴趣，又不想刷力扣，于是开始做 LoRA + Label Scoring（生成式分类接口）的实验。LoRA 算力要求其实很低，Claude Opus 4.6 太好用了你知道吗，我在十一点半前就改好代码跑起来了，只跑了五十多分钟就完成了。

让我惊讶的是，它的结果其实是最好最完美的，即使有着 Label / Verbalizer Bias（标签词偏置）的一定影响。这也许又一次让我对大模型的边界能力有了新的认知。

### 性能对比

| 指标 | qwen_lora | qwen_lora_label_scoring | 差异 |
|-------|-----------|-------------------------|------|
| **准确率** | 0.938 | **0.958** | +2.0% ✅ |
| **F1分数** | 0.937 | **0.958** | +2.1% ✅ |
| **精确率** | 0.971 | 0.959 | -1.2% |
| **召回率** | 0.906 | **0.957** | +5.1% ✅ |
| **负向召回** | 0.971 | 0.958 | -1.3% |
| **正向召回** | 0.906 | **0.957** | +5.1% ✅ |

### 原因分析

分类头是从隐藏状态到类别的映射，可能不够精细。

**Label Scoring** 训练时计算的条件概率，推理时也用同一个概率。**充分利用 LLM 的语义理解**——标签 token 本身有丰富的语义信息。

### 结论

Label Scoring 更符合"用 LLM 做 LLM 擅长的事"的设计哲学。准确率只提升 2%，但生成式方法更优雅、更符合 LLM 的设计初衷，最大程度地利用了模型预训练得到的所有信息。


