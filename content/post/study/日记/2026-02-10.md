---
title: 转码 Day 21：
date: 2026-02-10
categories:
  - diary
tag:
  - diary
math: true
---
lc：39、23

## MLA在干嘛
先生成一个“公共内容向量“ 
### $c_t^{KV}=W^{DKV}h_t$ 
- 所有 head **共享**
    
- 维度较小（低秩）
    
- **这是唯一需要缓存的大块 KV 内容**
每个 head 的 K/V 都从它“展开”

$[k_{t,1}^C,\ldots,k_{t,n_h}^C] = W^{UK}c_t^{KV}$

$[v_{t,1}^C,\ldots,v_{t,n_h}^C] = W^{UV}c_t^{KV}$

- 不缓存结果
    
- 推理时 **现算**
    
- 类似“用同一块内存，按不同视角投影”
MLA中的低秩公共内容向量和KV展开投影矩阵都是可训练参数，通过梯度下降学习，既减少参数量又保证多head注意力的个性化能力。
位置相关的 Key 单独保留（不压缩）

最终 Key = 内容 + 位置

MLA ≈ 所有注意力头共用一本“内容词典”，  
每个头只带一副“观察角度”和“方向感”，对所有token通用。
MLA 把  $W^Kh_t$ **人为拆成两块子空间**：
$W^Kh_t=\begin{bmatrix}k_t^C\\k_t^{(\mathrm{raw~}R)}\end{bmatrix}$

$\bullet$ $k_{t}^{C}:$不做 RoPE,走低秩压缩
$\bullet$ $k_{t}^{( \mathrm{raw~}R) }:$只做 RoPE,得到$k_{t}^{R}$

> **RoPE 的位置耦合，不体现在 K 本身，而体现在 Q·K 的内积里。**

注意力是内积，拼接是可加的

除了KV的缓存减少外，针对Q ，ds也进行了低秩压缩。
最终
$\begin{aligned}&\mathbf{0}_{t,i}=\sum_{j=1}^{t}\mathrm{Softmax}_{j}(\frac{\mathbf{q}_{t,i}^{T}\mathbf{k}_{j,i}}{\sqrt{d_{h}+d_{h}^{R}}})\mathbf{v}_{j,i}^{C},\\&\mathbf{u}_t=W^{O}[\mathbf{o}_{t,1};\mathbf{o}_{t,2};...;\mathbf{o}_{t,n_{h}}],\end{aligned}$

## 为什么 MLA 里还要强调 $u_t$
因为前面发生了两件非标准的事：
1. Key 被拆成 $[k^C; k^R]$
2. Value 只剩内容部分 $v^C$
但这里要强调：
这些改动只影响“注意力怎么算”，不影响“attention 层的接口语义”。
对外：
- 输入：$h_t$
- 输出：$u_t \in \mathbb{R}^d$
层与层之间完全无感知。


# DeepSeekMoE with Auxiliary-Loss-Free Load Balancing

DeepSeekMoE 使用更细粒度的专家，并将一些专家隔离为共享专家
$\begin{aligned}&\mathbf{h}_{t}=\mathbf{u}_{t}+\sum_{i=1}^{N_{s}}\mathrm{FFN}_{i}^{(s)}(\mathbf{u}_{t})+\sum_{i=1}^{N_{r}}g_{i,t}\mathrm{FFN}_{i}^{(r)}(\mathbf{u}_{t}),\\&g_{i,t}=\frac{g_{i,t}^{\prime}}{\sum_{j=1}^{N_{r}}g_{j,t}^{\prime}},\\&g_{i,t}^{\prime}=\begin{cases}s_{i,t},&s_{i,t}\in\mathrm{Topk}(\{s_{j,t}|1\leqslant j\leqslant N_{r}\},K_{r}),\\0,&\mathrm{otherwise},&\end{cases}\\&s_{i,t}=\mathrm{Sigmoid}\left(\mathbf{u}_{t}{}^{T}\mathbf{e}_{i}\right),\end{aligned}$

shared experts（Ns个）
- **每个 token 都会经过**
- 不需要 gating
- 作用：- 基础语法- 通用模式- 稳定低频能力
routed experts（Nr个）
- **只有被选中的少数 expert 有非零权重**
- 真正提供模型容量
- token 级动态选择
- 
expert 打分：si,t $s_{i,t}=\mathrm{Sigmoid}(\mathbf{u}_t^\top\mathbf{e}_i)$         - 输出 (0,1) - **不是概率，只是相关度**

Top-K 稀疏化 $g_{i,t}^{\prime}=\begin{cases}s_{i,t},&i\in\mathrm{TopK}(s_{\cdot,t})\\0,&\mathrm{otherwise}&\end{cases}$ 每个 token **只激活 Kr​ 个 expert**
为每个专家引入一个偏差项𝑏𝑖，并将其添加到相应的亲和力分数𝑠𝑖
$g_{i,t}^{\prime}=\begin{cases}s_{i,t},&s_{i,t}+b_i\in\mathrm{Topk}(\{s_{j,t}+b_j|1\leqslant j\leqslant N_r\},K_r),\\0,&\mathrm{otherwise}.&\end{cases}$

在 **Top-K 内归一化** $g_{i,t}=\frac{g_{i,t}^{\prime}}{\sum_jg_{j,t}^{\prime}}$
### 用 Sigmoid + Top-K，而不是 softmax
- softmax → 所有 expert 都有梯度
- Sigmoid + Top-K → **硬稀疏**
- 更像“选择专家”而不是“加权平均”
gated 部分是加权求和，增加非线性容量
这更接近：
> 条件计算（conditional computation）


DeepSeek-MoE = “Dense FFN + 稀疏专家增量”
对每个 token：  
一定走共享 FFN，  
再挑少数专长 FFN 叠加，  
gating 只决定“谁参与、参与多少”。

在训练过程中增加了超参数 𝛾。相应的 Expert 过载，我们将减少偏差项 𝛾，如果相应的 Expert 负载不足，将增加 𝛾。 通过动态调整，DeepSeek-V3在训练过程中保持专家负载平衡，并且比通过纯辅助损失鼓励负载平衡的模型取得了更好的性能。

在 MoE 中，token dropping 指的是：

> **当某些 expert 过载时，直接丢弃一部分 token，不送进任何 expert 计算。**

超参数 𝛾来调整过载，- expert 负载天然接近均匀 **不需要靠丢 token 来兜底**

MTP主要是为了提高主模型的性能。因此在推理时，我们可以直接丢弃MTP模块，主模型可以独立正常运行。 此外，我们还可以重新利用这些 MTP 模块进行推测解码，以进一步改善生成延迟。

看到infra部分的内容，过于抽象，偏离学习路径了，决定跳过。DualPipe改善设备间通信，Recomputation of RMSNorm and MLA Up-Projection.利用 FP8 数据格式训练 DeepSeek-V3 的细粒度混合精度框架。与 BF16 基线相比，FP8 训练模型的相对损失误差始终低于 0.25%，完全处于训练随机性可接受的范围内。

# Pre-Training

与DeepSeek-V2相比，通过提高数学和编程样本的比例来优化预训练语料，同时扩大英语和中文以外的多语言覆盖范围。
**只有 10% 的训练样本使用 FIM 格式**。 是“能力增强”，不是基础建模
middle 往往是：
- 一个函数体
- 一个逻辑块
- 一段实现细节
这对代码模型尤其关键。

顺便温习了一下RoPE里面的细节。当 m 增加时，**高维度对（大 i ） 因为周期长，能支持更长的上下文；而 低维度对（ 小 i )会先出现角度重复**，但由于是多维度组合，整体仍然能保持唯一性。
### 组合起来
关键洞察 ：所有维度对 同时重复 的周期是各周期的 最小公倍数
总周期 ≈ LCM ( 6 , 111 , 1984 , ... , 62832 ) ≈ 1000 0 d /2
这是一个天文数字，远超任何实际序列长度。

当位置m 很大时：

- 低维度对的旋转角度 m × θ i ​ 已经转了很多圈
- 绝对位置编码出现周期性重复
- 但是： 相对位置信息仍然保留
理论上：组合周期≈ $10000^{d/2}$ ，天文数字

实际上：模型只在 训练时的最大长度 内学习过
训练长度 = 4096
推理长度 = 100000
问题：
- RoPE能编码位置100000（编码能力没问题）
- 但注意力机制没见过这么长的依赖关系
- softmax的分布行为在超长距离上是"未定义"的
- 
问题：模型只见过0, 4096×θ_i范围内的角度
      现在突然看到50000×θ_i，完全超出分布
      
YaRN/NTK-aware的作用：不是修复RoPE的编码能力  而是调整RoPE，让模型能"理解"长距离

NTK-aware 缩放base（10000→更大）- 位置50000的角度 ≈ 原来位置4096的角度 容易注意力涣散
YaRN：缩放base + 调整温度 两者兼顾
$\mathrm{base^{\prime}}=\mathrm{base}\times\left(\frac{L_{\mathrm{target}}}{L_{\mathrm{train}}}\right)^{\frac{d}{d-2}}$
$\theta_{i}^{\prime}=(\mathrm{base}^{\prime})^{-2i/d}=\theta_{i}\times s^{-2i/d}$
$attention\_score = \frac{Q \cdot K^T}{\sqrt{d} \times t}$
$$t \approx 0.1 \times \log_{10}\left(\frac{L_{target}}{L_{train}}\right) + 1$$


tokenizer采用BBPE。
 AdamW 优化器
在最后 500B 个令牌的训练过程中，在前 333B 个令牌中保持 2.2 × 10^-5 的恒定学习率，并在剩余的 167B 个令牌中切换到另一个 7.3 × 10^−6 的恒定学习率。
利用管道并行性将模型的不同层部署在不同的 GPU 上，对于每一层，路由专家将统一部署在属于 8 个节点的 64 个 GPU 上

预训练阶段之后，应用 YaRN (Peng et al., 2023a) 进行上下文扩展，并执行两个额外的训练阶段，每个阶段包含 1000 个步骤，以逐步将上下文窗口从 4K 扩展到 32K，然后再扩展到 128K。

## ablation的结果理解

MTP 策略训练在大多数评估基准上持续提高了模型性能。
标准Transformer:
输入 → Transformer Layers → 输出层 → logits

MTP Transformer:
输入 → Transformer Layers → 共享表示
                              ↓
                    ┌────────┼────────┬────────┐
                    ↓        ↓        ↓        ↓
                  主头      头2      头3      头4
                 (t+1)    (t+2)    (t+3)    (t+4)



辅助无损失策略在大多数评估基准上始终取得更好的模型性能。



# Post-Training


对于与推理相关的数据集，包括那些专注于数学、代码竞争问题和逻辑难题的数据集，通过利用内部 DeepSeek-R1 模型来生成数据。

R1 生成的数据表现出很强的准确性，但它存在思考过度、格式不良和长度过长等问题。 目标是平衡 R1 生成的推理数据的高精度与规则格式的推理数据的清晰和简洁。

使用监督微调 (SFT) 和强化学习 (RL) 的组合训练管道，开发针对特定领域（例如代码、数学或一般推理）量身定制的专家模型。
对于非推理数据，例如创意写作、角色扮演和简单问答，利用 DeepSeek-V2.5 生成响应并聘请人工注释者来验证数据的准确性和正确性。

强化学习 奖励模型
 基于规则的 RM。 对于可以使用特定规则验证的问题，我们采用基于规则的奖励系统来确定反馈。
 基于模型的 RM。 对于具有自由形式的真实答案的问题，我们依靠奖励模型来确定响应是否与预期的真实答案相匹配。 相反，对于没有明确事实真相的问题，例如涉及创意写作的问题，奖励模型的任务是根据问题和相应答案作为输入提供反馈。

采用组相对策略优化 (GRPO) (Shao et al., 2024)，它放弃了通常与策略模型大小相同的批评家模型，而是根据组分数估计基线。 具体来说，对于每个问题 𝑞，GRPO 从旧策略模型 𝜋𝜃𝑜𝑙𝑑 中采样一组输出 {𝑜1, 𝑜2, ··· , 𝑜𝐺 }，然后通过最大化以下目标来优化策略模型𝜋𝜃：

$$\begin{aligned}\mathcal{J}_{GRPO}(\theta)&=\mathbb{E}[q\sim P(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{old}}(O|q)]\\&\frac{1}{G}\sum_{i=1}^{G}\left(\min\left(\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{old}}(o_{i}|q)}A_{i},\mathrm{clip}\left(\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{old}}(o_{i}|q)},1-\varepsilon,1+\varepsilon\right)A_{i}\right)-\beta\mathrm{D}_{KL}\left(\pi_{\theta}||\pi_{ref}\right)\right)\end{aligned}$$



强化学习过程中，我们结合了来自不同领域的提示，例如编码、数学、写作、角色扮演和问题回答。 这种方法不仅使模型更符合人类偏好，而且还提高了基准测试的性能，特别是在可用 SFT 数据有限的情况下。

在博士级评估测试平台GPQA-Diamond上，DeepSeek-V3成绩斐然，仅落后于Claude 3.5 Sonnet，大幅领先其他竞争对手。（再次期待v4，能达到opus4.6的水平）

DeepSeek-V3分配更多的训练标记来学习中文知识，从而在C-SimpleQA上取得优异的性能。故可以理解 在事实知识基准 SimpleQA 上，DeepSeek-V3 落后于 GPT-4o 和 Claude-Sonnet 3.5


在蒸馏R1模型之后 蒸馏可以带来更好的性能，但也大大增加了平均响应长度。


在推理的时候添加 Multi-Token Prediction，第二个token预测的接受率在 85% 到 90% 之间，表现出一致的可靠性。这种高接受率使 DeepSeek-V3 能够显着提高解码速度，提供 1.8 倍的 TPS（每秒令牌数）。

在参与人员名单上快速划过时无意撇到了fuliluo，前段时间了解过这个学姐。她2019年从北大读研毕业，现在是2026，不知道我2028毕业后，2035我会在哪里呢hhhhh。

FP8 对比 BF16 训练
 通过高精度累加和细粒度量化策略，相对误差保持在 0.25% 以下。


