---
title: "转码学习day8"
date: 2026-01-28
categories: 
  - study
tag: 
  - study
math: true 
---

lc:138、108、200（List[List[str]]没注意到，得用字符串判断语句，不能用整型判断）、34、198

今天微调了一下个人站，尽可能按着审美来，花了俩小时，没办法，不会写网页脚本和js，git-copilot运行又巨慢，等以后有钱了一定直接用cc或者cursor。

晚上做项目，注意不能写\# dataset = load_dataset(path = "NousResearch/hermes-function-calling-v1",split = "train")这种代码。

不然就算本地cache里面有，也会先联网看看能不能ping通，然后再确认是否本地缓存有，最后再load，直接用绝对路径path最好。

fast_tokenize用rust写的，slow_tokenize用python写的，如果模型自带的tokenize里有fast的话，默认使用tokenize就是fast的了，fast的里面。

教程model篇还没看，和朋友打海斗打了4把。

看了一下transformers如何优化显存。以hfl/chinese-macbert-large，330M为例。

baseline 32 maxlength128 ，4090占用15.2GB， 64s。

可以先计算32个batch之后，再进行参数优化。gradient_accumulation_steps =32,梯度累计，优化前向激活值。

如果采用BS1 GAS 32,显存占用可以直接掉一半，但计算时间变成260s，BS1破坏了显卡并行计算的优势。

gradient checkpoints设置，不保存所有前向的激活值，选择性存一些即可，没存的留到反向传播再重新计算即可。gradient_checkpointing = True。从BS1GAS32 的7.4变成7.2，优化不多，时间变成7分钟。优化前向激活值。

Adafactor Optimizer，optim = “adafactor”，来替代adam，adamw，变为5g显存，时间变长很少。它是对优化器本身的优化，不需要记录adam那么多一阶二阶梯度、动量之类的参数，来达到减少显存占用。

Freeze Model，冻结模型一些参数，比如文本分类任务由bert和全连接层构成的模型，冻结预训练的参数，只训练全连接层参数，降显存比较明显，训练速度也加快。但直观的感觉上来看，它绝对影响了训练效果，何时采用，见仁见智。显存来到3.4g占用，3分钟。优化的对象是梯度生成的过程

data length优化 ，原来都是max length128，变成32.优化的对象是前向激活值。非常非常影响模型效果，毕竟数据集长度本身就是其质量的一部分，训练效果不能保证。在小模型和短数据集上，效果其实约等于没有，因为对原本的数据集而言数据长度32也够了。





