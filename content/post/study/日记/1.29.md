---
title: "转码学习day9"
date: 2026-01-29
categories: 
  - study
tag: 
  - study
math: true 
---

今天打算直接开始学项目，天天下午刷力扣太浪费时间了，而且脑细胞死的快。力扣留到晚上睡前吧。

transformer架构每个decoder都收到了最后一个encoder的输出，而不是只有最后一层 decoder 才做 cross-attention，这是为了每个抽象阶段的decoder预测都能重新参考输入。在ablation中，如果只在最后一层做cross-attention，其他层不做，则条件利用率明显下降，更像在“润色”而不是“受条件生成”。

model-head,连接在模型后的层，输入128，输出768，最终输出维度1x128x768，hidden state经过不同head映射到不同的任务输出。

学习了一下Datasets库怎么导入划分数据集，怎么自定义dataloader，collate_fn函数怎么用或者自定义.怎么根据feature和要求划分数据巴拉巴拉。碰到feature和字段套了好几层的数据集，如何提取等等。单一层字段datasets.value,套了一层以上的字段则datasets.features.Sequence.

lc:45、55、1326

默写mha一遍，睡觉



