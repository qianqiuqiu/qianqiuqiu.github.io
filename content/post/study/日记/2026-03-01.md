---
title: 学习day40：起床晚，看面经
date: 2026-03-01
categories:
  - diary
tag:
  - diary
math: true
---
lc：70
## **vLLM 为什么能加速生成？**
核心技术是 **PagedAttention**：
传统 KV Cache 会给每个请求预分配一块连续显存，空间浪费严重（碎片化）。vLLM 借鉴操作系统虚拟内存的分页思想，把 KV Cache 切成小块动态分配，显存利用率大幅提升，从而支持更大 batch size。
在 GRPO 场景里，每步要对同一个 prompt 生成 G=4（或更多）个回复，**生成阶段是最大瓶颈**。

## 梯度检查点为什么能节省显存

标准训练中，反向传播需要用到每一层的**激活值（activations）**来计算梯度。对于一个 L 层的 Transformer，前向传播会把所有层的激活值全部保留在显存里，显存消耗与层数成正比

梯度检查点的做法
它节省**中间激活值**，而不是模型参数本身。

| 维度 | 梯度检查点 | 梯度累积（Gradient Accumulation） |
|------|------------|-----------------------------------|
| 作用维度 | 模型的深度方向（层与层之间） | 数据的批次方向（step 与 step 之间） |
| 解决的问题 | 单次前向产生的激活值太多，撑爆显存 | 单个 batch 太小，等效 batch size 不够大 |
| 机制 | 丢弃中间层激活值，反向时按需重算 | 多个小 batch 的梯度叠加，攒够再 optimizer.step() |
| 影响的是 | 峰值显存 | 等效 batch size / 梯度质量 |
二者不是同一个概念。
正向传播算loss，反向传播需要哪一层哪一个值，再按正常方式算出来，不存着。
检查点放得越密，显存越高、重算越少；放得越稀，显存越低、重算越多。


## PPO DPO 

| 维度 | PPO | DPO | GRPO |
|------|-----|-----|------|
| 更新的模型数 | Actor + Critic（2 个） | Actor（1 个） | Actor（1 个） |
| 冻结的模型数 | Reference + Reward Model | Reference | Reference |
| Critic 的选取自由 | 有，可独立设计 | 无（已被解析消去） | 无（用组内统计代替） |
PPO和GRPO都是在线的
在线 vs 离线的本质区别
关键不在于"拿谁的输出做比较"，而在于**训练时回复 y 从哪里来**。
PPO:取 prompt x
  2. 当前策略模型 π_θ 现场生成回复 y  ← 在线
  3. 奖励模型对 y 打分 → r
  4. 用 r 更新 π_θ
  5. π_θ 已经变了，下一步继续用新的 π_θ 生成
DPO：训练前（数据集制作阶段，与训练无关）：
  人类或另一个模型生成了 y_w（好回复）和 y_l（差回复）
  标注员确认偏好：y_w > y_l
  → 打包成 dataset，冻结
训练第 t 步：
  6. 从 dataset 取出固定的三元组 (x, y_w, y_l)  ← 离线
  7. 分别喂给 π_θ 和 π_ref，只做前向传播
  8. 计算 log ratio，更新 π_θ
  9. π_θ 更新了，但 dataset 里的 y_w, y_l 永远不变

DPO问题：当 πθ​ 训练到后期，分布已经漂移，模型在学习"其他人犯过的错误"而非"自己现在会犯的错误"，存在 **distribution shift**

**PPO 的优势**：每步都用当前策略采样，训练信号始终与当前模型分布对齐，但代价是必须在训练中反复做生成（推理），计算开销大



## SFT RLHF 本质区别

核心差异在于**损失函数告诉模型"学什么"的方式**。

SFT 的 loss 是标准的交叉熵（语言模型 loss）要求模型的输出分布**完全贴近训练数据里给定的那条回复**，每个 token 都要对准。本质是**行为克隆（Behavior Cloning）**——告诉模型"你就照着这个写"。

PPO/DPO 的 loss 本质上是在优化**跨回复的相对关系**

**PPO/GRPO**：用奖励信号缩放每条回复的 log-prob 梯度，好回复梯度加强，坏回复梯度抑制
SFT 是**单点监督**：告诉模型"这个输出是对的，往这里拟合"。  
RLHF 是**对比/序数监督**：告诉模型"这个输出比那个更好/更差，调整相对概率"——这个信号在 SFT 数据里是隐式缺失的。

**DPO** 的信号是**离散序数**（yw≻yl，只知道谁更好，不知道好多少），对的形式天然就是成对的。
**PPO** 的信号是**连续标量**（reward model 输出一个实数分），好多少/差多少都有量化，不需要成对，单条回复就能打分。

**GRPO** 在我的项目里**不是神经网络奖励模型**，而是规则函数（答案对不对，程序直接判断）。这类设计叫 **RLVR（RL with Verifiable Rewards）**，是 DeepSeek-R1-Zero 的核心创新点之一：完全绕开了奖励模型可能被 reward hacking 的问题，信号来源是客观事实而非模型打分。



过两天可以去看看alibi的做法，RoPE我刚刚才注意到原来就是jianlin su 大佬提出的，给跪了，之前一直看他的博客，但是没注意到他就是提出者。。。。。。




