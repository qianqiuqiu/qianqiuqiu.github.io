---
title: 学习day35：回学校了
date: 2026-02-24
categories:
  - diary
tag:
  - diary
math: true
---
lc：153、4
前天高中同学聚会，昨天一整天高铁赶路回北京去了，所以day33、34就消失了。
今天早上十点到北京，回到学校十一点多了，整理完东西洗个澡吃个午饭睡个午觉现在已经四点多了。打算这两天固化一下知识点，把力扣刷完然后过一遍，顺手投一些中小厂的实习去了。

Post train的训练模式 Tülu 3：开源后训练的标准，用系统性的消融实验给出了具有实验支撑的设置。

Qwen3 技术报告里说它们分了四阶段后训练，非常有参考价值。
阶段 1：长思维链冷启动 SFT Long-CoT Cold Start 让模型掌握 `<think>...</think>` 格式的思维链输出
阶段 2：推理 RL Reasoning Reinforcement Learning   在数学、代码等可验证任务上进行大规模 **GRPO** 训练，强化模型的推理能力。     关键效果：模型学会了更长、更深入的推理链，自发涌现出自我验证和回溯能力
阶段 3：思考模式融合Thinking Mode Fusion 推理 RL 阶段获得的深度思考能力**融合回统一模型**，使模型同时具备：
- **思考模式**（Thinking Mode）：生成详细的内部推理过程
- **非思考模式**（Non-Thinking Mode）：直接给出简洁回复
"✅ 以 SFT 为主体,🔁 但训练数据里包含 RL 专家模型生成的高质量轨迹"
阶段 4：通用 RL（General Reinforcement Learning）
使用通用奖励信号（包括人类偏好和规则奖励）进行最终的强化学习，全面提升模型在各项能力上的表现，包括指令跟随、安全性、多语言等。

Qwen3内置思考和不思考模式

用户：/think 解方程 3x + 7 = 22
助手：`<think>`我需要解方程 3x + 7 = 22。首先，将 7 从等式两边减去：3x + 7 - 7 = 22 - 73x = 15然后，将两边除以 3：x = 15 / 3x = 5让我验证一下：3 × 5 + 7 = 15 + 7 = 22 ✓`</think>`
解方程 3x + 7 = 22：将常数项移到右边：3x = 22 - 7 = 15两边除以 3：**x = 5**验证：3 × 5 + 7 = 22 ✓


用户：/no_think 解方程 3x + 7 = 22
助手： `<think>` `</think>`解方程 3x + 7 = 22：- 3x = 15- **x = 5**

“只有经过完整后训练流程的 **Instruct** 版本才支持 `/think` 和 `/no_think` 模式切换。基座模型（Base）不具备这一能力——这正是后训练的价值所在。”

SFT第一步是将指令-回复对格式化为模型能理解的输入格式，各家采用的不同，但学到的时候再查就好，大都不是本质区别。
ChatML（Chat Markup Language）是 Qwen3 系列采用的聊天模板格式。它使用特殊 token 标记每个角色的消息边界 ChatML 天然支持多轮对话，只需按顺序排列多轮 user/assistant 消息
Llama格式：Meta 的 Llama 系列使用不同的模板格式。

