---
title: 学习day37：继续学习，改简历，海投简历
date: 2026-02-26
categories:
  - diary
tag:
  - diary
math: true
---
lc：416

昨天学习了 DPO——一种跳过奖励模型和 RL 循环的偏好优化方法
尽管 DPO 在很多场景中可以替代 RLHF，但 RLHF 的**在线探索能力**在训练推理模型时不可替代。
理解 RLHF 也是理解 GRPO（下一节）的必要基础。

InstructGPT 三阶段流程
SFT  RM  PPO

奖励模型训练
奖励模型的训练基于 Bradley-Terry 偏好模型（与 DPO 使用的是同一个模型）
$$\mathcal{L}_{\mathrm{RM}} = - \mathbb{E}_{(x,y_w,y_l)\sim \mathcal{D}_{\mathrm{pref}}} 
\left[ \log \sigma \left( r_\phi(x,y_w) - r_\phi(x,y_l) \right) \right]$$
这个损失与 DPO 损失的形式非常相似！区别在于：RM 损失训练的是一个**独立的奖励模型** rϕrϕ​，而 DPO 损失直接用**策略模型的对数概率比**作为隐式奖励。
奖励模型通常基于与策略模型相同的预训练模型初始化，但将最后的语言模型头替换为一个**标量输出头**

|RM 质量|后果|
|---|---|
|高质量 RM|PPO 训练有效，模型对齐良好|
|有偏差的 RM|策略学到 RM 的偏差（如偏向长回复）|
|低质量 RM|奖励黑客严重，PPO 训练失败|

PPO目标函数
$$\max_{\pi_\theta} \ \mathbb{E}_{x \sim \mathcal{D}, \, y \sim \pi_\theta(\cdot \mid x)} 
\left[ r_\phi(x,y) \right] 
- \beta \, \mathrm{KL} \left[ \pi_\theta(y \mid x) \,\|\, \pi_{\mathrm{ref}}(y \mid x) \right]$$
与 DPO 推导的第一步完全一致 DPO 通过解析求解绕过了 RL，而 RLHF 直接用 PPO 算法来优化这个目标。

PPO 的核心创新是**截断代理目标 限制每步更新的幅度，确保训练稳定**
$$  
\mathcal{L}_{\mathrm{PPO}}  
= - \mathbb{E}_t  
\left[  
\min \left(  
\rho_t \hat{A}_t,  
\mathrm{clip}(\rho_t, 1 - \epsilon, 1 + \epsilon)\hat{A}_t  
\right)  
\right]  
$$
- 当 A^t>0（好的 action）：允许 ρt​ 增大但不超过 1+ϵ
- 当 A^t<0（坏的 action）：允许 ρt​ 减小但不低于 1−ϵ

GAE（Generalized Advantage Estimation）用于计算每个 token 的优势值
$$  
\hat{A}_t^{\mathrm{GAE}}  
= \sum_{l=0}^{T-t} (\gamma \lambda)^l \delta_{t+l}  
$$

$$  
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)  
$$
在 RLHF 的语境中：
- 每个 token 的生成是一个"action"
- "reward" 通常只在序列结束时给出（即 rϕ​(x,y) 是整个回复的奖励）
- V(st)是价值模型预测的中间状态价值
加入 KL 惩罚项
$$
r_{\mathrm{total}}(x,y)
= r_\phi(x,y)
- \beta \sum_{t}
\log
\frac{
\pi_\theta(y_t \mid x, y_{<t})
}{
\pi_{\mathrm{ref}}(y_t \mid x, y_{<t})
}
$$KL 惩罚可以按 token 计算（如上式），也可以作为整体约束。
PPO 在 RLHF 中需要**同时维护四个模型** 其计算开销巨大

常见不稳定性：reward hacking KL 散度爆炸 价值模型RM崩溃

|维度|RLHF (PPO)|DPO|
|---|---|---|
|核心思想|训练 RM → RL 优化策略|直接在偏好对上优化|
|模型数量|4 个|2 个|
|训练阶段|3 个（SFT → RM → PPO）|2 个（SFT → DPO）|
|实现复杂度|极高|低|
|训练稳定性|低（多种不稳定性）|高|
|计算资源|极高|中等|
|在线探索|有（关键优势）|无|
|数据效率|中等（需要生成 + 评估）|高（直接用偏好数据）|
|推理训练|适合（可探索新推理路径）|不适合|
|聊天对齐|过度工程（DPO 足够）|首选|

PPO到GRPO进步：
1. **消除价值模型**：用组统计量替代价值网络，减少一个模型
2. **消除奖励模型**（在 RLVR 场景下）：使用可验证的正确性奖励，再减少一个模型
3. **保留在线探索**：仍然从当前策略采样，保持 RL 的探索优势


DeepSeek-R1-Zero：纯 RL 的里程碑
一个**完全不经过 SFT**、仅通过强化学习训练的推理模型
- **思维链推理（Chain-of-Thought）**：逐步分析问题
- **自我验证（Self-Verification）**：在给出答案后检查自己的推理
- **回溯（Backtracking）**：发现错误后回到之前的步骤重来
- **"顿悟时刻"（Aha Moment）**：在推理过程中突然发现新的解题策略

 RL 可以涌现推理能力
模型对数学题生成多个回复 正确的回复获得 +1 奖励，错误的获得 0 
经过组内归一化后，逐步推理的回复获得正的优势值  更新增强逐步推理的生成概率 随着训练进行，模型发展出越来越精细的推理策略

GRPO 的核心洞察是：与其用一个独立的价值网络来估计基线，不如直接用**同一批采样的平均奖励**作为基线。

GRPO四步：
分组采样：多个采样 单次采样无法区分"回复好因为模型本身好"还是"只是运气好"。多次采样后取统计量，可以更准确地评估每个回复的相对质量。
计算奖励：
$$  
R(q, o) =  
\begin{cases}  
1, & \text{如果 } o \text{ 中的最终答案正确} \\  
0, & \text{如果 } o \text{ 中的最终答案错误}  
\end{cases}  
$$

可以加入格式奖励：如果回复包含 `<think>...</think>` 标签，额外奖励 +0.1。
 
组内归一化：
$$  
\hat{A}_i

\frac{  
r_i - \mathrm{mean}({r_1, \dots, r_G})  
}{  
\mathrm{std}({r_1, \dots, r_G})  
}  
$$
一个回复的奖励**高于**组平均值：A^i>0，应该被**强化**
一个回复的奖励**低于**组平均值：A^i​<0，应该被**抑制**

更新策略加入PPO截断+KL惩罚：
$$\mathcal{L}_{\mathrm{GRPO}}(\theta)=-\frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left[\min\left(\rho_{i,t}\hat{A}_i,\mathrm{~clip}(\rho_{i,t},1-\epsilon,1+\epsilon)\hat{A}_i\right)-\beta D_{\mathrm{KL}}^{(i,t)}\right]$$

| 方面 | PPO | GRPO |
| --- | --- | --- |
| 优势估计 | 价值网络 $V_{\psi} + \text{GAE}$ | 组统计量 $\hat{A}_i = (r_i - \mu)/\sigma$ |
| 模型数量 | 4个 | 2-3个（无价值模型） |
| 内存需求 | 极高 | 减少~50% |
| token 级/序列级 | token 级优势 | 序列级优势 |
| 基线准确度 | 较高（学习到的 $V$） | 较粗糙（组均值） |
| 实现复杂度 | 高 | 中 |
| 训练稳定性 | 低 | 中（无价值模型崩溃问题） |


**RLVR（Reinforcement Learning with Verifiable Rewards）** 是 GRPO 最自然的应用场景。当任务有**确定性标准答案**时，奖励函数就是简单的正确性检查

R1四阶段训练：冷启动SFT GRPO  拒绝采样+SFT 最终RL（进行最终的 RL 优化，包括通用对话、安全对齐)

为什么 RL 能涌现推理:
预训练阶段已经"见过"了大量的推理文本
RL通过奖励信号激活潜在能力 还优化推理策略
模型自发地发现了有效的推理格式

|超参数|含义|推荐值|影响|
|---|---|---|---|
|G (num_generations)|每个提示的采样数量|8–16|更大 → 更准确的基线估计，但计算成本更高|
|β (kl_coef)|KL 惩罚系数|0.001–0.01|更大 → 更保守，更小 → 更激进|
|ε (clip_range)|PPO 截断参数|0.1–0.2|更大 → 允许更大的更新步幅|
|lr (learning_rate)|学习率|1e-6 ~ 5e-6|更大 → 更快收敛但可能不稳定|
|max_new_tokens|最大生成 token 数|1024–2048|更大 → 允许更长的推理链|

GRPO改进：
相比 PPO 大幅简化了推理 RL 的训练流程，但是问题存在：
长度偏差、采样效率在于正误对比，当组内所有回复都正确或都错误时，优势为零，该批次的训练信号为零、稳定性有待提高、熵坍缩，探索能力下降

### DAPO
**DAPO（Decoupled Alignment Policy Optimization）**（Yu 等，字节跳动，2025）是对 GRPO 最全面的改进方案，包含四个核心技术：

clip higher 截断范围不对称，给好回复更大空间。

Dynamic sampling 当组内方差为零时，**重新采样**或**过滤掉该组**，只保留有信息量的训练样本，而不是将标准差设置为0

Token级损失归一化
标准 GRPO 对每个回复的损失除以回复长度 ∣oi​∣，这会导致**长回复和短回复被等权处理**。但实际上，长回复包含更多 token 级决策，应该有更大的贡献。
改为 token 级归一化——对整个 mini-batch 内的**所有 token 的损失总和**除以 **token 总数**，而非按序列归一化。
从seq级别归一化改成batch层级归一化

Long answer punishment超过最大长度的截断回复施加额外惩罚，鼓励模型学习更简洁的推理：
 $$R_{\mathrm{total}}=R_{\text{correctness}}+R_{\mathrm{format}}-\lambda\cdot1[|o|>L_{\max}]$$


Dr. GRPO：去除长度偏差 总的来说DAPO更系统。此处不深入


**REINFORCE++**： 可以理解为 REINFORCE（经典策略梯度算法）加上一些现代改进，但**不使用 Critic（价值网络）**
 REINFORCE++ 和 GRPO 的核心思想非常接近——都是**去掉 Critic、用简单基线替代**

一些可扩展讨论：
**Snell 等（2024）** 的研究提出了一个重要发现：
> 对于**中等难度**的问题，在推理时分配更多计算资源（让模型"思考更长时间"），可以超越**14倍大**的模型。

这意味着：**在某些场景下，增加推理计算比增加模型参数更有性价比。**


### 工程实现
RLHF 和 GRPO 的工程实现远比 SFT 复杂——需要管理多个模型、处理在线生成、协调训练和推理。开源社区已经构建了多个成熟的工具框架。

介绍四种主要工具
TRL huggingface官方库 与 Transformers、PEFT、Accelerate 深度集成。
局限：分布式支持不好、30B以上不如专门框架、自定义程度不高

OPENRLHF：基于 **Ray + vLLM** 的分布式 RLHF 框架，专为大规模训练设计。

veRL：之前就在github给它点过star，但一直没系统地看过它，之后有时间可以深入学习。字节跳动开源的 RLHF 框架，特别优化了 **GRPO 的大规模训练**。
veRL 的核心设计理念是**训练和推理的解耦（Decoupling）**：

- **推理集群**：使用 vLLM 或其他推理引擎，专门负责策略模型的在线生成
- **训练集群**：使用 FSDP/Megatron-LM，专门负责模型参数更新
- **两个集群之间通过 NCCL 高效通信**
这种解耦设计使得可以为推理和训练分别配置最优的并行策略。
veRL 适合：
- 工业级 GRPO 训练（30B+ 模型）
- 需要极致训练效率的场景
- 字节跳动技术栈的团队

REINFORCE++ via TRL RLOOTrainer
TRL 的 `RLOOTrainer` 提供的一个**轻量级替代方案**


考虑到晚上别人下班了，我下午就直接把简历改出来拿去投了一部分公司了，晚上把写到简历里的实验4做完放着跑，人去睡觉了。

实验4：迷你 DeepSeek-R1-Zero






